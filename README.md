## ⚡ Quick Start

1. **Review findings**: Check JSON files main directory
2. **Examine examples**: See JSON files main directory
3. **Run tests**: Use notebook in main directory
4. **Study methodology**: https://www.kaggle.com/competitions/openai-gpt-oss-20b-red-teaming/writeups/systematic-safety-failures-in-gpt-oss-20b

## 🎯 Impact Assessment

- **Severity**: Level 7-8 (Enables real cyberattacks, system damage)
- **Breadth**: Level 6-7 (Affects millions across technical domains)
- **Reproducibility**: Level 9 (Highly consistent across categories)

## 📊 Competition Submission

This research was submitted to the **OpenAI GPT-OSS-20B Red Teaming Competition** demonstrating systematic safety failures across 8 out of 9 vulnerability categories.

**Kaggle Notebook**: [Systematic Safety Failures Analysis](https://www.kaggle.com/code/saikumarallaka/systematic-safety-failures-in-gpt-oss-20b/notebook)

## 🛡️ Mitigation Recommendations

1. **Content-Agnostic Safety**: Apply same safety policies regardless of fictional framing
2. **Consistent Filtering**: Screen fictional content for real-world applicable information
3. **Policy Integration**: Ensure safety measures activate on content, not context

## ⚠️ Responsible Disclosure

This research is shared for AI safety improvement purposes. The vulnerabilities demonstrated pose real-world risks and should be addressed in production AI systems.

## 📄 License

MIT License - See LICENSE file for details

## 🤝 Contributing

Issues and pull requests welcome for extending this safety research framework.
